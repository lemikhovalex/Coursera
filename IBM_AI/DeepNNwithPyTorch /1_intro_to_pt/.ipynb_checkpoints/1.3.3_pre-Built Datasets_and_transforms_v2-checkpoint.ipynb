{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://cocl.us/pytorch_link_top\">\n",
    "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \" />\n",
    "</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Prebuilt Datasets and Transforms</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "<p>In this lab, you will use a prebuilt dataset and then use some prebuilt dataset transforms.</p>\n",
    "<ul>\n",
    "    <li><a href=\"#Prebuilt_Dataset\">Prebuilt Datasets</a></li>\n",
    "    <li><a href=\"#Torchvision\">Torchvision Transforms</a></li>\n",
    "</ul>\n",
    "<p>Estimated Time Needed: <strong>10 min</strong></p>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Preparation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7168d5e710>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the libraries will be used for this lab.\n",
    "\n",
    "import torch \n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function for displaying images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data by diagram\n",
    "\n",
    "def show_data(data_sample, shape = (28, 28)):\n",
    "    plt.imshow(data_sample[0].numpy().reshape(shape), cmap='gray')\n",
    "    plt.title('y = ' + str(data_sample[1].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Prebuilt_Dataset\">Prebuilt Datasets</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will focus on the following libraries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the command below when you do not have torchvision installed\n",
    "# !conda install -y torchvision\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can import a prebuilt dataset. In this case, use MNIST. You'll work with several of these parameters later by placing a transform object in the argument <code>transform</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Import the prebuilt dataset into variable dataset\n",
    "\n",
    "dataset = dsets.MNIST(\n",
    "    root = './data', \n",
    "    train = False, \n",
    "    download = True, \n",
    "    transform = transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element of the dataset object contains a tuple. Let us see whether the first element in the dataset is a tuple and what is in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of the first element:  <class 'tuple'>\n",
      "The length of the tuple:  2\n",
      "The shape of the first element in the tuple:  torch.Size([1, 28, 28])\n",
      "The type of the first element in the tuple <class 'torch.Tensor'>\n",
      "The second element in the tuple:  tensor(7)\n",
      "The type of the second element in the tuple:  <class 'torch.Tensor'>\n",
      "As the result, the structure of the first element in the dataset is (tensor([1, 28, 28]), tensor(7)).\n"
     ]
    }
   ],
   "source": [
    "# Examine whether the elements in dataset MNIST are tuples, and what is in the tuple?\n",
    "\n",
    "print(\"Type of the first element: \", type(dataset[0]))\n",
    "print(\"The length of the tuple: \", len(dataset[0]))\n",
    "print(\"The shape of the first element in the tuple: \", dataset[0][0].shape)\n",
    "print(\"The type of the first element in the tuple\", type(dataset[0][0]))\n",
    "print(\"The second element in the tuple: \", dataset[0][1])\n",
    "print(\"The type of the second element in the tuple: \", type(dataset[0][1]))\n",
    "print(\"As the result, the structure of the first element in the dataset is (tensor([1, 28, 28]), tensor(7)).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the output, the first element in the tuple is a cuboid tensor. As you can see, there is a dimension with only size 1, so basically, it is a rectangular tensor.<br>\n",
    "The second element in the tuple is a number tensor, which indicate the real number the image shows. As the second element in the tuple is <code>tensor(7)</code>, the image should show a hand-written 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the first element in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the first element in the dataset\n",
    "\n",
    "show_data(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it is a 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the second sample:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the second element in the dataset\n",
    "\n",
    "show_data(dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Torchvision\"> Torchvision Transforms  </h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply some image transform functions on the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, the images in the MNIST dataset can be cropped and converted to a tensor. We can use <code>transform.Compose</code> we learned from the previous lab to combine the two transform functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine two transforms: crop and convert to tensor. Apply the compose to MNIST dataset\n",
    "\n",
    "croptensor_data_transform = transforms.Compose([transforms.CenterCrop(20), transforms.ToTensor()])\n",
    "dataset = dsets.MNIST(root = './data', train = False, download = True, transform = croptensor_data_transform)\n",
    "print(\"The shape of the first element in the first tuple: \", dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the image is now 20 x 20 instead of 28 x 28."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the first image again. Notice that the black space around the <b>7</b> become less apparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first element in the dataset\n",
    "\n",
    "show_data(dataset[0],shape = (20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the second element in the dataset\n",
    "\n",
    "show_data(dataset[1],shape = (20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below example, we horizontally flip the image, and then convert it to a tensor. Use <code>transforms.Compose()</code> to combine these two transform functions. Plot the flipped image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the compose. Apply it on MNIST dataset. Plot the image out.\n",
    "\n",
    "fliptensor_data_transform = transforms.Compose([transforms.RandomHorizontalFlip(p = 1),transforms.ToTensor()])\n",
    "dataset = dsets.MNIST(root = './data', train = False, download = True, transform = fliptensor_data_transform)\n",
    "show_data(dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Practice</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to use the <code>RandomVerticalFlip</code> (vertically flip the image) with horizontally flip and convert to tensor as a compose. Apply the compose on image. Use <code>show_data()</code> to plot the second image (the image as <b>2</b>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice: Combine vertical flip, horizontal flip and convert to tensor as a compose. Apply the compose on image. Then plot the image\n",
    "\n",
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- \n",
    "my_data_transform = transforms.Compose([transforms.RandomVerticalFlip(p = 1), transforms.RandomHorizontalFlip(p = 1), transforms.ToTensor()])\n",
    "dataset = dsets.MNIST(root = './data', train = False, download = True, transform = my_data_transform)\n",
    "show_data(dataset[1])\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://cocl.us/pytorch_link_bottom\">\n",
    "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/notebook_bottom%20.png\" width=\"750\" alt=\"PyTorch Bottom\" />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>About the Authors:</h2> \n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other contributors: <a href=\"https://www.linkedin.com/in/michelleccarey/\">Michelle Carey</a>, <a href=\"www.linkedin.com/in/jiahui-mavis-zhou-a4537814a\">Mavis Zhou</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2018 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_i",
   "language": "python",
   "name": "venv_i"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
