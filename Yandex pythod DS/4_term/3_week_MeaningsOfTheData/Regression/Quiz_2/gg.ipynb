{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Поиск закономерностей в данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation. Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть ли связь между неграмотностью и рождаемостью? Для 94 стран, уровень неграмотности женщин в которых больше 5%, известны доля неграмотных среди женщин старше 15 (на 2003 год) и средняя рождаемость на одну женщину (на 2005 год).\n",
    "\n",
    "Чему равен выборочный коэффициент корреляции Пирсона между этими двумя признаками? Округлите до четырёх знаков после десятичной точки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File illiteracy.txt does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-54cb7e90680b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0milliteracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'illiteracy.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Country'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0milliteracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lemikhovalex/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lemikhovalex/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lemikhovalex/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lemikhovalex/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lemikhovalex/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File illiteracy.txt does not exist"
     ]
    }
   ],
   "source": [
    "illiteracy = pd.read_csv('illiteracy.txt', sep = '\\t', index_col = 'Country')\n",
    "illiteracy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_answer(df, method = 'pearson'):\n",
    "    return round(float(df.corr(method = method).iloc[0,1]), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pearson Correlation =', corr_answer(illiteracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чему равен выборочный коэффициент корреляции Спирмена признаков из предыдущего вопроса? Округлите до четырёх знаков после десятичной точки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Spearman Correlation =', corr_answer(illiteracy, method = 'spearman'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation. Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для 61 большого города в Англии и Уэльсе известны средняя годовая смертность на 100000 населения (по данным 1958–1964) и концентрация кальция в питьевой воде (в частях на маллион). Чем выше концентрация кальция, тем жёстче вода. Города дополнительно поделены на северные и южные.\n",
    "\n",
    "Есть ли связь между жёсткостью воды и средней годовой смертностью? Посчитайте значение коэффициента корреляции Пирсона между этими признаками, округлите его до четырёх знаков после десятичной точки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water = pd.read_csv('water.txt', sep = '\\t')\n",
    "water.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pearson Correlation =', corr_answer(water[['mortality', 'hardness']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В предыдущей задаче посчитайте значение коэффициента корреляции Спирмена между средней годовой смертностью и жёсткостью воды. Округлите до четырёх знаков после десятичной точки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Spearman Correlation =', corr_answer(water[['mortality', 'hardness']], method = 'spearman'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняется ли связь между признаками, если разбить выборку на северные и южные города? Посчитайте значения корреляции Пирсона между средней годовой смертностью и жёсткостью воды в каждой из двух подвыборок, введите наименьшее по модулю из двух значений, округлив его до четырёх знаков после десятичной точки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "north_cor = np.abs(corr_answer(water[(water['location'] == 'North')][['mortality', 'hardness']]))\n",
    "south_cor = np.abs(corr_answer(water[(water['location'] == 'South')][['mortality', 'hardness']]))\n",
    "\n",
    "print('ABS Minimum Perason Correlation =', round(float(np.minimum(north_cor, south_cor)), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Среди респондентов General Social Survey 2014 года хотя бы раз в месяц проводят вечер в баре 203 женщины и 239 мужчин; реже, чем раз в месяц, это делают 718 женщин и 515 мужчин.\n",
    "\n",
    "Посчитайте значение коэффициента корреляции Мэтьюса между полом и частотой похода в бары. Округлите значение до трёх знаков после десятичной точки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 0 | 1  \n",
    "------------- | -------------|\n",
    "0  | a | b \n",
    "1  | c | d \n",
    "\n",
    "$$\\text{Формула корреляции Мэтьюса:}$$\n",
    "  \n",
    "$$MCC_{X_1, X_2} =  \\frac{ad-bc}{\\sqrt{(a+b)(a+c)(b+d)(c+d)}}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 0 | 1  \n",
    "------------- | -------------|\n",
    "0  | 203 | 239 \n",
    "1  | 718 | 515 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcor = (203*515 - 718*239)/np.sqrt((203+239)*(203+718)*(515+239)*(515+718))\n",
    "\n",
    "print('Matthew Correlation =', round(mcor,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В предыдущей задаче проверьте, значимо ли коэффициент корреляции Мэтьюса отличается от нуля. Посчитайте достигаемый уровень значимости; используйте функцию scipy.stats.chi2_contingency. Введите номер первой значащей цифры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "bar = np.array([[203, 239], [718, 515]])\n",
    "\n",
    "print('p-value =', chi2_contingency(bar)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В предыдущей задаче давайте попробуем ответить на немного другой вопрос: отличаются ли доля мужчин и доля женщин, относительно часто проводящих вечера в баре? Постройте 95% доверительный интервал для разности долей, вычитая долю женщин из доли мужчин. Чему равна его нижняя граница? Округлите до четырёх знаков после десятичной точки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fem = np.append(np.ones(203), np.zeros(718))\n",
    "mal = np.append(np.ones(239), np.zeros(515))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportions_diff_confint_ind(sample1, sample2, alpha = 0.05):    \n",
    "    z = scipy.stats.norm.ppf(1 - alpha / 2.)\n",
    "    \n",
    "    p1 = float(sum(sample1)) / len(sample1)\n",
    "    p2 = float(sum(sample2)) / len(sample2)\n",
    "    \n",
    "    left_boundary = (p1 - p2) - z * np.sqrt(p1 * (1 - p1)/ len(sample1) + p2 * (1 - p2)/ len(sample2))\n",
    "    right_boundary = (p1 - p2) + z * np.sqrt(p1 * (1 - p1)/ len(sample1) + p2 * (1 - p2)/ len(sample2))\n",
    "    \n",
    "    return (left_boundary, right_boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"95%% confidence interval for a difference between proportions: [%f, %f]\" %\\\n",
    "      proportions_diff_confint_ind(mal, fem))\n",
    "\n",
    "print('Answer is', round(proportions_diff_confint_ind(mal, fem)[0], 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте гипотезу о равенстве долей любителей часто проводить вечера в баре среди мужчин и женщин. Посчитайте достигаемый уровень значимости, используя двустороннюю альтернативу. Введите номер первой значащей цифры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportions_diff_z_stat_ind(sample1, sample2):\n",
    "    n1 = len(sample1)\n",
    "    n2 = len(sample2)\n",
    "    \n",
    "    p1 = float(sum(sample1)) / n1\n",
    "    p2 = float(sum(sample2)) / n2 \n",
    "    P = float(p1*n1 + p2*n2) / (n1 + n2)\n",
    "    \n",
    "    return (p1 - p2) / np.sqrt(P * (1 - P) * (1. / n1 + 1. / n2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportions_diff_z_test(z_stat, alternative = 'two-sided'):\n",
    "    if alternative not in ('two-sided', 'less', 'greater'):\n",
    "        raise ValueError(\"alternative not recognized\\n\"\n",
    "                         \"should be 'two-sided', 'less' or 'greater'\")\n",
    "    \n",
    "    if alternative == 'two-sided':\n",
    "        return 2 * (1 - scipy.stats.norm.cdf(np.abs(z_stat)))\n",
    "    \n",
    "    if alternative == 'less':\n",
    "        return scipy.stats.norm.cdf(z_stat)\n",
    "\n",
    "    if alternative == 'greater':\n",
    "        return 1 - scipy.stats.norm.cdf(z_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"p-value: %f\" % proportions_diff_z_test(proportions_diff_z_stat_ind(mal, fem)))\n",
    "print('Answer is', 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на данные General Social Survey 2014 года и проанализируем, как связаны ответы на вопросы \"Счастливы ли вы?\" и \"Довольны ли вы вашим финансовым положением?\" Чему равно значение статистики хи-квадрат для этой таблицы сопряжённости? Округлите ответ до четырёх знаков после десятичной точки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Не доволен|Более или менее|Доволен\n",
    "---|---|---\n",
    "Не очень счастлив|197|111|33\n",
    "Достаточно счастлив|382|685|331\n",
    "Очень счастлив|110|\t342|333\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy = np.array([[197, 111, 33], [382, 685, 331], [110, 342, 333]])\n",
    "\n",
    "print('Chi2 Stats =', round(float(chi2_contingency(happy)[0]), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На данных из предыдущего вопроса посчитайте значение достигаемого уровня значимости. Введите номер первой значащей цифры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('p-value =', chi2_contingency(happy)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чему в предыдущей задаче равно значение коэффициента V Крамера для рассматриваемых признаков? Округлите ответ до четырёх знаков после десятичной точки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\phi_c(X^{n}_1, X^{n}_2) = \\sqrt{\\frac{ \\chi^2(X^{n}_1, X^{n}_2)}{n(min(K_1, K_2) - 1)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cramer = np.sqrt(chi2_contingency(happy)[0]/(np.sum(happy) * 2))\n",
    "\n",
    "print('Cramer Correlation =', round(cramer, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Множественная проверка гипотез. Quiz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Классификатор C4.5 и три его модификации: с оптимизацией гиперпараметра m, гиперпараметра cf и с одновременной оптимизацией обоих гиперпараметров. Эти четыре классификатора сравнивались на 14 наборах данных. На каждом датасете был посчитан AUC каждого классификатора. \n",
    "\n",
    "Используя критерий знаковых рангов, проведите попарное сравнение каждого классификатора с каждым. Выберите два классификатора, различие между которыми наиболее статистически значимо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucs = pd.read_csv('AUCs.txt', sep = '\\t', index_col = 0)\n",
    "\n",
    "aucs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "wilcox_data = []\n",
    "\n",
    "for i, lhs_column in enumerate(aucs.columns):\n",
    "    for j, rhs_column in enumerate(aucs.columns):\n",
    "        if i >= j:\n",
    "            continue\n",
    "        \n",
    "        stat, p = stats.wilcoxon(aucs[lhs_column], aucs[rhs_column])\n",
    "        wilcox_data.append([lhs_column, rhs_column, stat, p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C4.5, C4.5+m\n",
    "wilcox_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Сколько статистически значимых на уровне 0.05 различий мы обнаружили? **4**\n",
    "\n",
    "### Question 4\n",
    "\n",
    "Судя по данным из предыдущего опроса, настройка какого из параметров классификатора даёт более значимое увеличение качества? **m**\n",
    "\n",
    "### Question 5\n",
    "\n",
    "Сравнивая 4 классификатора между собой, мы проверили 6 гипотез. Давайте сделаем поправку на множественную проверку. Начнём с метода Холма. Сколько гипотез можно отвергнуть на уровне значимости 0.05 после поправки этим методом? **0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.sandbox.stats.multicomp import multipletests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_wilcox = pd.DataFrame.from_records(wilcox_data)\n",
    "models_wilcox.columns = ['model_A', 'model_B', 'wilcox_stats', 'p']\n",
    "\n",
    "models_wilcox.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reject, p_corrected, a1, a2 = multipletests(models_wilcox.p, \n",
    "                                            alpha = 0.05, \n",
    "                                            method = 'holm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_wilcox['p_corrected'] = p_corrected\n",
    "models_wilcox['reject'] = reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_wilcox.reject.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Сколько гипотез можно отвергнуть на уровне значимости 0.05 после поправки методом Бенджамини-Хохберга? **3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reject, p_corrected, a1, a2 = multipletests(models_wilcox.p, \n",
    "                                            alpha = 0.05, \n",
    "                                            method = 'fdr_bh') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_wilcox['p_corrected'] = p_corrected\n",
    "models_wilcox['reject'] = reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_wilcox.reject.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практика построения регрессии. Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Давайте проанализируем данные опроса 4361 женщин из Ботсваны:\n",
    "\n",
    "О каждой из них мы знаем:\n",
    "\n",
    "* сколько детей она родила (признак ceb)\n",
    "* возраст (age)\n",
    "* длительность получения образования (educ)\n",
    "* религиозная принадлежность (religion)\n",
    "* идеальное, по её мнению, количество детей в семье (idlnchld)\n",
    "* была ли она когда-нибудь замужем (evermarr)\n",
    "* возраст первого замужества (agefm)\n",
    "* длительность получения образования мужем (heduc)\n",
    "* знает ли она о методах контрацепции (knowmeth)\n",
    "* использует ли она методы контрацепции (usemeth)\n",
    "* живёт ли она в городе (urban)\n",
    "* есть ли у неё электричество, радио, телевизор и велосипед (electric, radio, tv, bicycle)\n",
    "\n",
    "Давайте научимся оценивать количество детей ceb по остальным признакам.\n",
    "\n",
    "Загрузите данные и внимательно изучите их. Сколько разных значений принимает признак religion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ceb</th>\n",
       "      <th>age</th>\n",
       "      <th>educ</th>\n",
       "      <th>religion</th>\n",
       "      <th>idlnchld</th>\n",
       "      <th>knowmeth</th>\n",
       "      <th>usemeth</th>\n",
       "      <th>evermarr</th>\n",
       "      <th>agefm</th>\n",
       "      <th>heduc</th>\n",
       "      <th>urban</th>\n",
       "      <th>electric</th>\n",
       "      <th>radio</th>\n",
       "      <th>tv</th>\n",
       "      <th>bicycle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>catholic</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>11</td>\n",
       "      <td>protestant</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>spirit</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>other</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>other</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ceb  age  educ    religion  idlnchld  knowmeth  usemeth  evermarr  agefm  \\\n",
       "0    0   18    10    catholic       4.0       1.0      1.0         0    NaN   \n",
       "1    2   43    11  protestant       2.0       1.0      1.0         1   20.0   \n",
       "2    0   49     4      spirit       4.0       1.0      0.0         1   22.0   \n",
       "3    0   24    12       other       2.0       1.0      0.0         0    NaN   \n",
       "4    3   32    13       other       3.0       1.0      1.0         1   24.0   \n",
       "\n",
       "   heduc  urban  electric  radio   tv  bicycle  \n",
       "0    NaN      1       1.0    1.0  1.0      1.0  \n",
       "1   14.0      1       1.0    1.0  1.0      1.0  \n",
       "2    1.0      1       1.0    1.0  0.0      0.0  \n",
       "3    NaN      1       1.0    1.0  1.0      1.0  \n",
       "4   12.0      1       1.0    1.0  1.0      1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "botswana = pd.read_csv('botswana.tsv', sep = '\\t')\n",
    "\n",
    "botswana.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spirit        1841\n",
       "other         1080\n",
       "protestant     993\n",
       "catholic       447\n",
       "Name: religion, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4\n",
    "botswana.religion.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Во многих признаках есть пропущенные значения. Сколько объектов из 4361 останется, если выбросить все, содержащие пропуски?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (4361, 15)\n",
      "Shape without NA: (1834, 15)\n",
      "Difference = 2527\n"
     ]
    }
   ],
   "source": [
    "#1834\n",
    "print('Shape: {}\\nShape without NA: {}\\nDifference = {}'.format(\n",
    "    botswana.shape, botswana.dropna().shape, botswana.shape[0]-botswana.dropna().shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "В разных признаках пропуски возникают по разным причинам и должны обрабатываться по-разному.\n",
    "\n",
    "Например, в признаке agefm пропуски стоят только там, где evermarr=0, то есть, они соответствуют женщинам, никогда не выходившим замуж. Таким образом, для этого признака NaN соответствует значению \"не применимо\".\n",
    "\n",
    "1. Создайте признак nevermarr, равный единице там, где в agefm пропуски.\n",
    "2. Удалите признак evermarr — в сумме с nevermarr он даёт константу, значит, в нашей матрице $X$ будет мультиколлинеарность.\n",
    "3. Замените NaN в признаке agefm на $c_{agefm}=0$\n",
    "4. У объектов, где nevermarr = 1, замените NaN в признаке heduc на $c_{heduc_1}=-1$ (ноль использовать нельзя, так как он уже встречается у некоторых объектов выборки).\n",
    "\n",
    "Сколько осталось пропущенных значений в признаке heduc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "botswana['nevermarr'] = np.where(botswana['agefm'].isnull(), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "botswana.drop(['evermarr'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "botswana['agefm'] = np.where(botswana['agefm'].isnull(), 0, botswana['agefm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "botswana['heduc'] = np.where((botswana['heduc'].isnull()) & (botswana['nevermarr'] == 1), -1, botswana['heduc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ceb</th>\n",
       "      <th>age</th>\n",
       "      <th>educ</th>\n",
       "      <th>religion</th>\n",
       "      <th>idlnchld</th>\n",
       "      <th>knowmeth</th>\n",
       "      <th>usemeth</th>\n",
       "      <th>agefm</th>\n",
       "      <th>heduc</th>\n",
       "      <th>urban</th>\n",
       "      <th>electric</th>\n",
       "      <th>radio</th>\n",
       "      <th>tv</th>\n",
       "      <th>bicycle</th>\n",
       "      <th>nevermarr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>catholic</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>11</td>\n",
       "      <td>protestant</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>spirit</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>other</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>other</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ceb  age  educ    religion  idlnchld  knowmeth  usemeth  agefm  heduc  \\\n",
       "0    0   18    10    catholic       4.0       1.0      1.0    0.0   -1.0   \n",
       "1    2   43    11  protestant       2.0       1.0      1.0   20.0   14.0   \n",
       "2    0   49     4      spirit       4.0       1.0      0.0   22.0    1.0   \n",
       "3    0   24    12       other       2.0       1.0      0.0    0.0   -1.0   \n",
       "4    3   32    13       other       3.0       1.0      1.0   24.0   12.0   \n",
       "\n",
       "   urban  electric  radio   tv  bicycle  nevermarr  \n",
       "0      1       1.0    1.0  1.0      1.0          1  \n",
       "1      1       1.0    1.0  1.0      1.0          0  \n",
       "2      1       1.0    1.0  0.0      0.0          0  \n",
       "3      1       1.0    1.0  1.0      1.0          1  \n",
       "4      1       1.0    1.0  1.0      1.0          0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "botswana.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('heduc NaNs left =', 123)\n"
     ]
    }
   ],
   "source": [
    "print('heduc NaNs left =', np.sum(botswana.heduc.isnull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Избавимся от оставшихся пропусков.\n",
    "\n",
    "Для признаков idlnchld, heduc и usemeth проведите операцию, аналогичную предыдущей: создайте индикаторы пропусков по этим признакам (idlnchld_noans, heduc_noans, usemeth_noans), замените пропуски на нехарактерные значения ($c_{idlnchld}=-1$, $c_{heduc_2}=-2$ (значение -1 мы уже использовали), $c_{usemeth}=-1$).\n",
    "\n",
    "Остались только пропуски в признаках knowmeth, electric, radio, tv и bicycle. Их очень мало, так что удалите объекты, на которых их значения пропущены.\n",
    "\n",
    "Какого размера теперь наша матрица данных? Умножьте количество строк на количество всех столбцов (включая отклик ceb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "botswana['idlnchld_noans'] = np.where(botswana['idlnchld'].isnull(), 1, 0)\n",
    "botswana['heduc_noans'] = np.where(botswana['heduc'].isnull(), 1, 0)\n",
    "botswana['usemeth_noans'] = np.where(botswana['usemeth'].isnull(), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "botswana['idlnchld'] = np.where(botswana['idlnchld'].isnull(), -1, botswana['idlnchld'])\n",
    "botswana['heduc'] = np.where(botswana['heduc'].isnull(), -2, botswana['heduc'])\n",
    "botswana['usemeth'] = np.where(botswana['usemeth'].isnull(), -1, botswana['usemeth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "botswana.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('New Shape =', 78264)\n"
     ]
    }
   ],
   "source": [
    "print('New Shape =', botswana.shape[0]*botswana.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Постройте регрессию количества детей ceb на все имеющиеся признаки методом smf.ols, как в разобранном до этого примере. Какой получился коэффициент детерминации $R^2$? Округлите до трёх знаков после десятичной точки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.api as sms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'ceb', u'age', u'educ', u'religion', u'idlnchld', u'knowmeth',\n",
       "       u'usemeth', u'agefm', u'heduc', u'urban', u'electric', u'radio', u'tv',\n",
       "       u'bicycle', u'nevermarr', u'idlnchld_noans', u'heduc_noans',\n",
       "       u'usemeth_noans'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "botswana.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    ceb   R-squared:                       0.644\n",
      "Model:                            OLS   Adj. R-squared:                  0.643\n",
      "Method:                 Least Squares   F-statistic:                     412.5\n",
      "Date:                Tue, 08 Oct 2019   Prob (F-statistic):               0.00\n",
      "Time:                        11:24:33   Log-Likelihood:                -7732.1\n",
      "No. Observations:                4348   AIC:                         1.550e+04\n",
      "Df Residuals:                    4328   BIC:                         1.563e+04\n",
      "Df Model:                          19                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==========================================================================================\n",
      "                             coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------\n",
      "Intercept                 -1.0263      0.212     -4.835      0.000      -1.443      -0.610\n",
      "religion[T.other]         -0.0830      0.083     -1.001      0.317      -0.245       0.080\n",
      "religion[T.protestant]    -0.0149      0.082     -0.181      0.857      -0.176       0.146\n",
      "religion[T.spirit]        -0.0191      0.077     -0.248      0.804      -0.171       0.132\n",
      "age                        0.1703      0.003     51.891      0.000       0.164       0.177\n",
      "educ                      -0.0724      0.007     -9.843      0.000      -0.087      -0.058\n",
      "idlnchld                   0.0760      0.011      6.923      0.000       0.054       0.098\n",
      "knowmeth                   0.5564      0.121      4.580      0.000       0.318       0.795\n",
      "usemeth                    0.6473      0.048     13.424      0.000       0.553       0.742\n",
      "agefm                     -0.0604      0.007     -9.213      0.000      -0.073      -0.048\n",
      "heduc                     -0.0551      0.008     -6.838      0.000      -0.071      -0.039\n",
      "urban                     -0.2137      0.047     -4.527      0.000      -0.306      -0.121\n",
      "electric                  -0.2685      0.077     -3.479      0.001      -0.420      -0.117\n",
      "radio                     -0.0235      0.051     -0.461      0.645      -0.123       0.076\n",
      "tv                        -0.1451      0.093     -1.566      0.118      -0.327       0.037\n",
      "bicycle                    0.2139      0.050      4.260      0.000       0.115       0.312\n",
      "nevermarr                 -2.2393      0.148    -15.143      0.000      -2.529      -1.949\n",
      "idlnchld_noans             0.6539      0.153      4.286      0.000       0.355       0.953\n",
      "heduc_noans               -0.8724      0.145     -6.026      0.000      -1.156      -0.589\n",
      "usemeth_noans              0.7652      0.196      3.910      0.000       0.382       1.149\n",
      "==============================================================================\n",
      "Omnibus:                      224.411   Durbin-Watson:                   1.887\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              859.014\n",
      "Skew:                           0.003   Prob(JB):                    2.93e-187\n",
      "Kurtosis:                       5.178   Cond. No.                         361.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "m1 = smf.ols('ceb ~ age + educ + religion + idlnchld + knowmeth + usemeth + agefm + heduc + urban + electric + radio + tv + bicycle + nevermarr + idlnchld_noans + heduc_noans + usemeth_noans', data = botswana)\n",
    "fitted = m1.fit()\n",
    "print(fitted.summary())\n",
    "\n",
    "#0.644"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Проверьте критерием Бройша-Пагана гомоскедастичность ошибки в построенной модели. Выполняется ли она?\n",
    "\n",
    "Если ошибка гетероскедастична, перенастройте модель, сделав поправку Уайта типа HC1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breusch-Pagan test: p=0.000000\n"
     ]
    }
   ],
   "source": [
    "print('Breusch-Pagan test: p=%f' % sms.het_breuschpagan(fitted.resid, fitted.model.exog)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>ceb</td>       <th>  R-squared:         </th> <td>   0.644</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.643</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   345.0</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 08 Oct 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:24:33</td>     <th>  Log-Likelihood:    </th> <td> -7732.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  4348</td>      <th>  AIC:               </th> <td>1.550e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  4328</td>      <th>  BIC:               </th> <td>1.563e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    19</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>         <td>HC1</td>       <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "             <td></td>               <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>              <td>   -1.0263</td> <td>    0.266</td> <td>   -3.863</td> <td> 0.000</td> <td>   -1.547</td> <td>   -0.506</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>religion[T.other]</th>      <td>   -0.0830</td> <td>    0.078</td> <td>   -1.067</td> <td> 0.286</td> <td>   -0.235</td> <td>    0.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>religion[T.protestant]</th> <td>   -0.0149</td> <td>    0.078</td> <td>   -0.192</td> <td> 0.848</td> <td>   -0.167</td> <td>    0.137</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>religion[T.spirit]</th>     <td>   -0.0191</td> <td>    0.071</td> <td>   -0.268</td> <td> 0.789</td> <td>   -0.159</td> <td>    0.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>                    <td>    0.1703</td> <td>    0.004</td> <td>   38.627</td> <td> 0.000</td> <td>    0.162</td> <td>    0.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>educ</th>                   <td>   -0.0724</td> <td>    0.007</td> <td>   -9.924</td> <td> 0.000</td> <td>   -0.087</td> <td>   -0.058</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>idlnchld</th>               <td>    0.0760</td> <td>    0.015</td> <td>    5.236</td> <td> 0.000</td> <td>    0.048</td> <td>    0.104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>knowmeth</th>               <td>    0.5564</td> <td>    0.174</td> <td>    3.190</td> <td> 0.001</td> <td>    0.215</td> <td>    0.898</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>usemeth</th>                <td>    0.6473</td> <td>    0.052</td> <td>   12.478</td> <td> 0.000</td> <td>    0.546</td> <td>    0.749</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>agefm</th>                  <td>   -0.0604</td> <td>    0.010</td> <td>   -6.174</td> <td> 0.000</td> <td>   -0.080</td> <td>   -0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>heduc</th>                  <td>   -0.0551</td> <td>    0.009</td> <td>   -6.126</td> <td> 0.000</td> <td>   -0.073</td> <td>   -0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>urban</th>                  <td>   -0.2137</td> <td>    0.046</td> <td>   -4.667</td> <td> 0.000</td> <td>   -0.303</td> <td>   -0.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>electric</th>               <td>   -0.2685</td> <td>    0.072</td> <td>   -3.732</td> <td> 0.000</td> <td>   -0.410</td> <td>   -0.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>radio</th>                  <td>   -0.0235</td> <td>    0.053</td> <td>   -0.446</td> <td> 0.656</td> <td>   -0.127</td> <td>    0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tv</th>                     <td>   -0.1451</td> <td>    0.082</td> <td>   -1.766</td> <td> 0.077</td> <td>   -0.306</td> <td>    0.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bicycle</th>                <td>    0.2139</td> <td>    0.048</td> <td>    4.412</td> <td> 0.000</td> <td>    0.119</td> <td>    0.309</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nevermarr</th>              <td>   -2.2393</td> <td>    0.202</td> <td>  -11.082</td> <td> 0.000</td> <td>   -2.635</td> <td>   -1.843</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>idlnchld_noans</th>         <td>    0.6539</td> <td>    0.216</td> <td>    3.029</td> <td> 0.002</td> <td>    0.231</td> <td>    1.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>heduc_noans</th>            <td>   -0.8724</td> <td>    0.191</td> <td>   -4.556</td> <td> 0.000</td> <td>   -1.248</td> <td>   -0.497</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>usemeth_noans</th>          <td>    0.7652</td> <td>    0.213</td> <td>    3.590</td> <td> 0.000</td> <td>    0.347</td> <td>    1.183</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>224.411</td> <th>  Durbin-Watson:     </th> <td>   1.887</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 859.014</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.003</td>  <th>  Prob(JB):          </th> <td>2.93e-187</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.178</td>  <th>  Cond. No.          </th> <td>    361.</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors are heteroscedasticity robust (HC1)"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    ceb   R-squared:                       0.644\n",
       "Model:                            OLS   Adj. R-squared:                  0.643\n",
       "Method:                 Least Squares   F-statistic:                     345.0\n",
       "Date:                Tue, 08 Oct 2019   Prob (F-statistic):               0.00\n",
       "Time:                        11:24:33   Log-Likelihood:                -7732.1\n",
       "No. Observations:                4348   AIC:                         1.550e+04\n",
       "Df Residuals:                    4328   BIC:                         1.563e+04\n",
       "Df Model:                          19                                         \n",
       "Covariance Type:                  HC1                                         \n",
       "==========================================================================================\n",
       "                             coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------\n",
       "Intercept                 -1.0263      0.266     -3.863      0.000      -1.547      -0.506\n",
       "religion[T.other]         -0.0830      0.078     -1.067      0.286      -0.235       0.069\n",
       "religion[T.protestant]    -0.0149      0.078     -0.192      0.848      -0.167       0.137\n",
       "religion[T.spirit]        -0.0191      0.071     -0.268      0.789      -0.159       0.121\n",
       "age                        0.1703      0.004     38.627      0.000       0.162       0.179\n",
       "educ                      -0.0724      0.007     -9.924      0.000      -0.087      -0.058\n",
       "idlnchld                   0.0760      0.015      5.236      0.000       0.048       0.104\n",
       "knowmeth                   0.5564      0.174      3.190      0.001       0.215       0.898\n",
       "usemeth                    0.6473      0.052     12.478      0.000       0.546       0.749\n",
       "agefm                     -0.0604      0.010     -6.174      0.000      -0.080      -0.041\n",
       "heduc                     -0.0551      0.009     -6.126      0.000      -0.073      -0.037\n",
       "urban                     -0.2137      0.046     -4.667      0.000      -0.303      -0.124\n",
       "electric                  -0.2685      0.072     -3.732      0.000      -0.410      -0.128\n",
       "radio                     -0.0235      0.053     -0.446      0.656      -0.127       0.080\n",
       "tv                        -0.1451      0.082     -1.766      0.077      -0.306       0.016\n",
       "bicycle                    0.2139      0.048      4.412      0.000       0.119       0.309\n",
       "nevermarr                 -2.2393      0.202    -11.082      0.000      -2.635      -1.843\n",
       "idlnchld_noans             0.6539      0.216      3.029      0.002       0.231       1.077\n",
       "heduc_noans               -0.8724      0.191     -4.556      0.000      -1.248      -0.497\n",
       "usemeth_noans              0.7652      0.213      3.590      0.000       0.347       1.183\n",
       "==============================================================================\n",
       "Omnibus:                      224.411   Durbin-Watson:                   1.887\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              859.014\n",
       "Skew:                           0.003   Prob(JB):                    2.93e-187\n",
       "Kurtosis:                       5.178   Cond. No.                         361.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors are heteroscedasticity robust (HC1)\n",
       "\"\"\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2 = smf.ols('ceb ~ age + educ + religion + idlnchld + knowmeth + usemeth + agefm + heduc + urban + electric + radio + tv + bicycle + nevermarr + idlnchld_noans + heduc_noans + usemeth_noans', data = botswana)\n",
    "fitted = m2.fit(cov_type='HC1')\n",
    "fitted.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Удалите из модели незначимые признаки religion, radio и tv. Проверьте гомоскедастичность ошибки, при необходимости сделайте поправку Уайта.\n",
    "\n",
    "Не произошло ли значимого ухудшения модели после удаления этой группы признаков? Проверьте с помощью критерия Фишера. Чему равен его достигаемый уровень значимости? Округлите до четырёх цифр после десятичной точки.\n",
    "\n",
    "Если достигаемый уровень значимости получился маленький, верните все удалённые признаки; если он достаточно велик, оставьте модель без религии, тв и радио."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3 = smf.ols('ceb ~ age + educ + idlnchld + knowmeth + usemeth + agefm + heduc + urban + electric + bicycle + nevermarr + idlnchld_noans + heduc_noans + usemeth_noans', data = botswana)\n",
    "fitted = m3.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breusch-Pagan test: p=0.000000\n"
     ]
    }
   ],
   "source": [
    "print('Breusch-Pagan test: p=%f' % sms.het_breuschpagan(fitted.resid, fitted.model.exog)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "m4 = smf.ols('ceb ~ age + educ + idlnchld + knowmeth + usemeth + agefm + heduc + urban + electric + bicycle + nevermarr + idlnchld_noans + heduc_noans + usemeth_noans', data = botswana)\n",
    "fitted = m4.fit(cov_type='HC1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F=0.919236, p=0.467231, k1=5.000000\n"
     ]
    }
   ],
   "source": [
    "#0.4672 Модель не стала хуже\n",
    "print(\"F=%f, p=%f, k1=%f\" % m2.fit().compare_f_test(m4.fit()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "Признак usemeth_noans значим по критерию Стьюдента, то есть, при его удалении модель значимо ухудшится. Но вообще-то отдельно его удалять нельзя: из-за того, что мы перекодировали пропуски в usemeth произвольно выбранным значением $c_{usemeth }=-1$, удалять usemeth_noans и usemeth можно только вместе.\n",
    "\n",
    "Удалите из текущей модели usemeth_noans и usemeth. Проверьте критерием Фишера гипотезу о том, что качество модели не ухудшилось. Введите номер первой значащей цифры в достигаемом уровне значимости.\n",
    "\n",
    "Если достигаемый уровень значимости получился маленький, верните удалённые признаки; если он достаточно велик, оставьте модель без usemeth и usemeth_noans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "m5 = smf.ols('ceb ~ age + educ + idlnchld + knowmeth + agefm + heduc + urban + electric + bicycle + nevermarr + idlnchld_noans + heduc_noans', data = botswana)\n",
    "fitted = m5.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F=0.919236, p=0.467231, k1=5.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"F=%f, p=%f, k1=%f\" % m2.fit().compare_f_test(m4.fit()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.155200948041877e-40"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#40\n",
    "m4.fit().compare_f_test(m5.fit())[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "Посмотрите на доверительные интервалы для коэффициентов итоговой модели (не забудьте использовать поправку Уайта, если есть гетероскедастичность ошибки) и выберите правильные выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>ceb</td>       <th>  R-squared:         </th> <td>   0.644</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.643</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   463.4</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 08 Oct 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:24:34</td>     <th>  Log-Likelihood:    </th> <td> -7734.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  4348</td>      <th>  AIC:               </th> <td>1.550e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  4333</td>      <th>  BIC:               </th> <td>1.559e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    14</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>         <td>HC1</td>       <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>           <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>      <td>   -1.0698</td> <td>    0.258</td> <td>   -4.152</td> <td> 0.000</td> <td>   -1.575</td> <td>   -0.565</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>            <td>    0.1702</td> <td>    0.004</td> <td>   38.746</td> <td> 0.000</td> <td>    0.162</td> <td>    0.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>educ</th>           <td>   -0.0729</td> <td>    0.007</td> <td>  -10.311</td> <td> 0.000</td> <td>   -0.087</td> <td>   -0.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>idlnchld</th>       <td>    0.0770</td> <td>    0.014</td> <td>    5.323</td> <td> 0.000</td> <td>    0.049</td> <td>    0.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>knowmeth</th>       <td>    0.5610</td> <td>    0.174</td> <td>    3.224</td> <td> 0.001</td> <td>    0.220</td> <td>    0.902</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>usemeth</th>        <td>    0.6516</td> <td>    0.052</td> <td>   12.571</td> <td> 0.000</td> <td>    0.550</td> <td>    0.753</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>agefm</th>          <td>   -0.0606</td> <td>    0.010</td> <td>   -6.192</td> <td> 0.000</td> <td>   -0.080</td> <td>   -0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>heduc</th>          <td>   -0.0573</td> <td>    0.009</td> <td>   -6.440</td> <td> 0.000</td> <td>   -0.075</td> <td>   -0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>urban</th>          <td>   -0.2190</td> <td>    0.045</td> <td>   -4.814</td> <td> 0.000</td> <td>   -0.308</td> <td>   -0.130</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>electric</th>       <td>   -0.3207</td> <td>    0.063</td> <td>   -5.076</td> <td> 0.000</td> <td>   -0.445</td> <td>   -0.197</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bicycle</th>        <td>    0.2046</td> <td>    0.048</td> <td>    4.279</td> <td> 0.000</td> <td>    0.111</td> <td>    0.298</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nevermarr</th>      <td>   -2.2501</td> <td>    0.202</td> <td>  -11.158</td> <td> 0.000</td> <td>   -2.645</td> <td>   -1.855</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>idlnchld_noans</th> <td>    0.6565</td> <td>    0.216</td> <td>    3.043</td> <td> 0.002</td> <td>    0.234</td> <td>    1.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>heduc_noans</th>    <td>   -0.8853</td> <td>    0.191</td> <td>   -4.638</td> <td> 0.000</td> <td>   -1.259</td> <td>   -0.511</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>usemeth_noans</th>  <td>    0.7732</td> <td>    0.212</td> <td>    3.639</td> <td> 0.000</td> <td>    0.357</td> <td>    1.190</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>224.096</td> <th>  Durbin-Watson:     </th> <td>   1.886</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 856.760</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.004</td>  <th>  Prob(JB):          </th> <td>9.06e-187</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.175</td>  <th>  Cond. No.          </th> <td>    345.</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors are heteroscedasticity robust (HC1)"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    ceb   R-squared:                       0.644\n",
       "Model:                            OLS   Adj. R-squared:                  0.643\n",
       "Method:                 Least Squares   F-statistic:                     463.4\n",
       "Date:                Tue, 08 Oct 2019   Prob (F-statistic):               0.00\n",
       "Time:                        11:24:34   Log-Likelihood:                -7734.5\n",
       "No. Observations:                4348   AIC:                         1.550e+04\n",
       "Df Residuals:                    4333   BIC:                         1.559e+04\n",
       "Df Model:                          14                                         \n",
       "Covariance Type:                  HC1                                         \n",
       "==================================================================================\n",
       "                     coef    std err          z      P>|z|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------\n",
       "Intercept         -1.0698      0.258     -4.152      0.000      -1.575      -0.565\n",
       "age                0.1702      0.004     38.746      0.000       0.162       0.179\n",
       "educ              -0.0729      0.007    -10.311      0.000      -0.087      -0.059\n",
       "idlnchld           0.0770      0.014      5.323      0.000       0.049       0.105\n",
       "knowmeth           0.5610      0.174      3.224      0.001       0.220       0.902\n",
       "usemeth            0.6516      0.052     12.571      0.000       0.550       0.753\n",
       "agefm             -0.0606      0.010     -6.192      0.000      -0.080      -0.041\n",
       "heduc             -0.0573      0.009     -6.440      0.000      -0.075      -0.040\n",
       "urban             -0.2190      0.045     -4.814      0.000      -0.308      -0.130\n",
       "electric          -0.3207      0.063     -5.076      0.000      -0.445      -0.197\n",
       "bicycle            0.2046      0.048      4.279      0.000       0.111       0.298\n",
       "nevermarr         -2.2501      0.202    -11.158      0.000      -2.645      -1.855\n",
       "idlnchld_noans     0.6565      0.216      3.043      0.002       0.234       1.079\n",
       "heduc_noans       -0.8853      0.191     -4.638      0.000      -1.259      -0.511\n",
       "usemeth_noans      0.7732      0.212      3.639      0.000       0.357       1.190\n",
       "==============================================================================\n",
       "Omnibus:                      224.096   Durbin-Watson:                   1.886\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              856.760\n",
       "Skew:                           0.004   Prob(JB):                    9.06e-187\n",
       "Kurtosis:                       5.175   Cond. No.                         345.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors are heteroscedasticity robust (HC1)\n",
       "\"\"\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m4 = smf.ols('ceb ~ age + educ + idlnchld + knowmeth + usemeth + agefm + heduc + urban + electric + bicycle + nevermarr + idlnchld_noans + heduc_noans + usemeth_noans', data = botswana)\n",
    "fitted = m4.fit(cov_type='HC1')\n",
    "fitted.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
